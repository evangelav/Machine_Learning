{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BINV3100 ML S4 - Exercices Model Selection, Cross Validation, Pre-Processing .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evangelav/Machine_Learning/blob/main/BINV3100_ML_S4_Exercices_Model_Selection%2C_Cross_Validation%2C_Pre_Processing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL83i7MKcPLX"
      },
      "source": [
        "#BINV3100 ML S4 : Théorie\n",
        "\n",
        "##Vidéos obligatoires\n",
        "Nous vous demandons de regarder et de compredre les vidéos suivante:\n",
        "\n",
        "1.   [SKLEARN : MODEL SELECTION](https://www.youtube.com/watch?v=w_bLGK4Pteo&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=21)\n",
        "2.   [CROSS VALIDATION](https://www.youtube.com/watch?v=VoyMOVfCSfc&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=22)\n",
        "3.   [MÉTRIQUE DE RÉGRESSION](https://www.youtube.com/watch?v=_TE9fDgtOaE&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=23)\n",
        "4.   [PRE-PROCESSING](https://www.youtube.com/watch?v=OGWwzm304Xs&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=24)\n",
        "5.   [IMPUTER : NETTOYAGE DE DONNÉES](https://www.youtube.com/watch?v=QVEJJNsz-eM&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=26)\n",
        "\n",
        "\n",
        "A la fin de chaque vidéo, un exercice est proposé. Vous retrouverez ces exercices dans la Partie 1 des exercices de cette semaine\n",
        "\n",
        "## Les vidéos suivantes sont en bonus:\n",
        "\n",
        "6.  [PIPELINE AVANCÉE](https://www.youtube.com/watch?v=41mnga4ptso&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js5X-FAlxHxg"
      },
      "source": [
        "# Partie 1: Exercices relatifs aux Vidéos Machine Learnia\n",
        "\n",
        "[Lien vers la playlist Machine Learnia](https://www.youtube.com/playlist?list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhUx-sL3xhBm"
      },
      "source": [
        "### Vidéos (21/30): \n",
        "[SKLEARN : MODEL SELECTION](https://www.youtube.com/watch?v=w_bLGK4Pteo&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=21)\n",
        "\n",
        "Vous allez créer un modèle afin de prédir les survivants du naufrage du titanic.\n",
        "\n",
        "Pour ce faire, \n",
        "\n",
        "\n",
        "1.   Ne gardez que les colones 'survived', 'pclass', 'sex', et 'age'\n",
        "2.   Supprimez les valeurs de type NaN\n",
        "3.   Dans la colonne sexe, remplacez 'male' et 'female' par 0 et 1\n",
        "4.   Pour votre dataSet, mettez la colonne 'survived' dans une variable y et les autres colonnes dans une variable X\n",
        "5.   Créez un Train Set et un Test Set avec le Test Set contenant 20% des données (Vous pouvez utiliser un random_state pour garder les mêmes Set à chaque exécution)\n",
        "6.   Utilisez un KNeighborsClassifier avec un nombre de voisin égal à 4, pour créer un premier modèle. \n",
        "7.   Entrainez-le\n",
        "8.   Évaluez-le avec un cross-validation avec 5 divisions.\n",
        "8.   A l'aide de GridSearchCV, trouvez les meilleurs hyper-paramètres n_neighbors (entre 1 et 20), metrics (entre 'euclidian' et 'Manhattan') et weights (entre 'uniform' et 'distance')\n",
        "9.   Gardez le meilleur modèle obtenu dans une variable best_model\n",
        "10.  A l'aide de learning_curve, regardez si collecter plus de données pourrait être utile pour améliorer encore ce modèle.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCrnvB8-cNFy"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN0ixxrp6NWi"
      },
      "source": [
        "## Vidéos (22 et 23 /30) : \n",
        "\n",
        "[CROSS VALIDATION](https://www.youtube.com/watch?v=VoyMOVfCSfc&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=22)\n",
        "\n",
        "[MÉTRIQUE DE RÉGRESSION](https://www.youtube.com/watch?v=_TE9fDgtOaE&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=23)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17UzKOPECNCO"
      },
      "source": [
        "## Vidéos (24 à 26/30): \n",
        "\n",
        "[PRE-PROCESSING](https://www.youtube.com/watch?v=OGWwzm304Xs&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=24)\n",
        "\n",
        "(Bonus) [PIPELINE AVANCÉE](https://www.youtube.com/watch?v=41mnga4ptso&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=25)\n",
        "\n",
        "[IMPUTER : NETTOYAGE DE DONNÉES](https://www.youtube.com/watch?v=QVEJJNsz-eM&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=26)\n",
        "\n",
        "Vous allez tester les différents Scaler sur le dataset des iris.\n",
        "\n",
        "Pour ce faire\n",
        "<ol>\n",
        "<li> Importer le dataset des iris </li>\n",
        "<li> Créez un Train Set et un Test Set avec le Test Set représentant 25% des données et random_State égal à 4 </li>\n",
        "<li> Pour chacun des 3 scalers (StandardScaler(), MinMaxScaler et RobustScaler)</li>\n",
        "<ol type=\"a\">\n",
        "<li>Créez un pipeline avec un PolynomialFeatures et un SGDClassifier (avec un random_state égal à 5 pour le Classifier) </li>\n",
        "<li>Cherchez les meilleurs paramètres à l'aide de GridSearchCV testant 2,3,4 pour le dégré du PolynomialFeatures, et \"l1\", \"l2\" pour la pénalité du SGDClassifier</li>\n",
        "<li>Afficher les meilleurs paramètres trouvé, le meilleur score ainsi que le score sur les données de test </li>\n",
        "</ol>\n",
        "<li>Sur base des résultats affichés au point 3., Quel scaler choisiriez-vous ? Justifiez !!</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybrn0Ct6d9Ur"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer,KNNImputer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xEq3wcYIvfg"
      },
      "source": [
        "Réponse au 4. : \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzqxLKjXOknb"
      },
      "source": [
        "# Partie 2 : Exercices Machine Learning semaine 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aApsbbFQ_eH"
      },
      "source": [
        "## Exercice 1 : Survivant du titanic : \n",
        "\n",
        "<ol>\n",
        "<li> Importez le dataset du titanic à l'aide de seaborn </li>\n",
        "<li> Gardez uniquement les colonnes 'pclass', 'age' et 'sex' comme features et la colonne 'survived' comme target </li>\n",
        "<li> Créez un Train Set et un Test set avec 20% des données pour le test set et un random_state égal à 4</li>\n",
        "<li> Créez un preprocesser qui </li>\n",
        "<ul>\n",
        "<li> pour la colonne 'pclass' remplace les valeurs manquantes en utilisant un KNNImputer() considérant les 2 plus proches voisins pour ensuite appliquer un OneHotEncoder </li>\n",
        "<li> pour la colonne 'age' remplace les valeurs manquantes par la moyenne et ensuite applique ensuite un RobustScaler </li>\n",
        "<li> pour la colonne 'sex' remplace les valeurs manquantes par la valeur la plus fréquente pour ensuite appliquer un OneHotEncoder</li>\n",
        "</ul>\n",
        "(Astuce : utilisez make_column_transformer)\n",
        "<li> Obtenez votre modèle en faisant un pipeline constitué du preprocesser suivi d'un SGDClassifier avec un random_state égal à 4 </li>\n",
        "<li> Cherchez les meilleurs paramètres du SGDClassifier à l'aide d'un GridSearchCV testant 'l1' et 'l2' comme pénalité. </li>\n",
        "<li> Afficher le meilleur paramètre, le meilleur score et le score obtenu sur les données de test.  </li>\n",
        "<li> Essayer d'améliorer le score obtenu en 7., en essayant d'autre encoder, scaler, classifier, ...\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GdotomCEON"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder,RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}